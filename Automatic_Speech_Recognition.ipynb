{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn6eo1JzAfHg",
        "outputId": "84eff041-78d6-402c-d59f-e04a74a64ef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIRtUpipArE1",
        "outputId": "38918b86-4293-4ff2-9bd6-e35a9b927fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1102 (delta 16), reused 32 (delta 14), pack-reused 1063\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 782.27 KiB | 11.17 MiB/s, done.\n",
            "Resolving deltas: 100% (529/529), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 14142, done.        \n",
            "remote: Counting objects: 100% (455/455), done.        \n",
            "remote: Compressing objects: 100% (318/318), done.        \n",
            "remote: Total 14142 (delta 149), reused 393 (delta 123), pack-reused 13687        \n",
            "Receiving objects: 100% (14142/14142), 5.91 MiB | 11.54 MiB/s, done.\n",
            "Resolving deltas: 100% (8029/8029), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content/ctcdecode\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb --quiet\n",
        "!pip install python-Levenshtein -q\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget -q\n",
        "%cd ctcdecode\n",
        "!pip install . -q\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xObg7ykfA6Jw"
      },
      "outputs": [],
      "source": [
        "!pip install torchsummaryX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXMI8ndSA-Ka"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import ConcatDataset\n",
        "from glob import glob\n",
        "import torchaudio.transforms as tat\n",
        "import torchvision \n",
        "import torchaudio\n",
        "import wandb\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "import string\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJguPoMRBlEC"
      },
      "outputs": [],
      "source": [
        " #TODO: Use the same Kaggle code from HW1P2\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"adritadas\",\"key\":\"4d8e5e73b5126973227f2e938ba1249f\"}') \n",
        "    # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDRe-_wZBoKS"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c 11-785-s23-hw3p2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qESp-ZzWBrYp"
      },
      "outputs": [],
      "source": [
        "!unzip -q 11-785-s23-hw3p2.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuXqfoiwDPH0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ItOfwdvDauw"
      },
      "outputs": [],
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "# This overwrites the phonetics.py file.\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
        "}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict[:-2]\n",
        "LABELS = ARPAbet[:-2]\n",
        "print(len(PHONEMES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph3joGekDfMK"
      },
      "outputs": [],
      "source": [
        "root = \"/content/11-785-s23-hw3p2\"\n",
        "partition = \"dev-clean\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG7IO1fxDisg"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX9JnDgaDmtv"
      },
      "outputs": [],
      "source": [
        "mfcc_dir = sorted(glob(os.path.join(root, partition, 'mfcc','*.npy')))\n",
        "transcript_dir = sorted(glob(os.path.join(root,partition,'transcript','*.npy')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2D4vqMoDpYp"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self,  root, phonemes = PHONEMES, partition= \"train-clean-360\" ): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "\n",
        "        #self.mfcc_dir = mfcc_dir\n",
        "        #self.transcript_dir = transcript_dir\n",
        "\n",
        "        self.mfcc_files = sorted(glob(os.path.join(root, partition, \"mfcc\", \"*.npy\")))\n",
        "        self.transcript_files = sorted(glob(os.path.join(root, partition, \"transcript\", \"*.npy\")))\n",
        "\n",
        "        self.PHONEMES = PHONEMES\n",
        "        self.label_to_int = {label: i for i, label in enumerate(self.PHONEMES)}\n",
        "        self.features = []\n",
        "        self.labels = []\n",
        "        for i, transcript_file in enumerate(self.transcript_files):\n",
        "            mfcc_file = self.mfcc_files[i]\n",
        "\n",
        "            assert os.path.splitext(os.path.basename(transcript_file))[0] == os.path.splitext(os.path.basename(mfcc_file))[0]\n",
        "\n",
        "            mfcc = np.load(mfcc_file)\n",
        "            transcript = np.load(transcript_file)[1:-1]\n",
        "            transcript = [self.label_to_int[p] for p in transcript]\n",
        "\n",
        "            # Normalize the mfcc features\n",
        "            mfcc -= np.mean(mfcc, axis=0)\n",
        "            mfcc /= (np.std(mfcc, axis=0) + 1e-8)\n",
        "\n",
        "            self.features.append(mfcc)\n",
        "            self.labels.append(transcript)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "       \n",
        "\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "       \n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return len(self.features)\n",
        "        \n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "        mfcc = torch.tensor(self.features[ind], dtype=torch.float)\n",
        "        transcript = torch.tensor(self.labels[ind], dtype=torch.long)\n",
        "       \n",
        "        return mfcc, transcript\n",
        "        \n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        mfcc, transcript = zip(*batch)\n",
        "        mfccs_padded = torch.nn.utils.rnn.pad_sequence(mfcc, batch_first=True)\n",
        "        transcripts_padded = torch.nn.utils.rnn.pad_sequence(transcript, batch_first=True)\n",
        "        mfcc_lengths = torch.tensor([len(mfcc) for mfcc in mfcc])\n",
        "        transcript_lengths = torch.tensor([len(transcript) for transcript in transcript])\n",
        "        return mfccs_padded, transcripts_padded, mfcc_lengths, transcript_lengths\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "\n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2zMIv8EDu-4"
      },
      "outputs": [],
      "source": [
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, phonemes=PHONEMES, partition=\"test-clean\"):\n",
        "        \"\"\"\n",
        "        Initializes the test dataset.\n",
        "        Inputs:\n",
        "            root: str\n",
        "                Path to the root directory containing data.\n",
        "            partition: str\n",
        "                Name of the partition. Default is \"test-clean\".\n",
        "        \"\"\"\n",
        "        self.mfcc_files = sorted(glob(os.path.join(root, partition, \"mfcc\", \"*.npy\")))\n",
        "        self.length = len(self.mfcc_files)\n",
        "        self.PHONEMES = PHONEMES\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the test dataset.\n",
        "        \"\"\"\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Loads a single MFCC file from disk and returns it as a PyTorch tensor.\n",
        "        \"\"\"\n",
        "        mfcc_path = self.mfcc_files[idx]\n",
        "        mfcc = np.load(mfcc_path)\n",
        "        mfcc -= np.mean(mfcc, axis=0)\n",
        "        mfcc /= (np.std(mfcc, axis=0) + 1e-8)\n",
        "        mfcc_tensor = torch.tensor(mfcc, dtype=torch.float)\n",
        "       \n",
        "        return mfcc_tensor\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"\n",
        "        Pads the sequences in the batch and returns the padded batch along with the lengths.\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        mfcc_tensors = [t for t in batch]\n",
        "        mfccs_padded = torch.nn.utils.rnn.pad_sequence(mfcc_tensors, batch_first=True)\n",
        "        mfcc_lengths = torch.tensor([len(mfcc) for mfcc in mfcc_tensors])\n",
        "        return mfccs_padded,  mfcc_lengths\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f85FyaUuDz6z"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64 # Increase if your device can handle it\n",
        "\n",
        "#transforms = torchvision.transforms.Compose([\n",
        "    #torchaudio.transforms.Spectrogram(),\n",
        "    #torchaudio.transforms.TimeMasking(time_mask_param = 80),\n",
        "    #torchaudio.transforms.Normalize(),\n",
        "\n",
        "\n",
        "# set of tranformations\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "\n",
        "root = '/content/11-785-s23-hw3p2' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB5_g9sPD3Q6"
      },
      "outputs": [],
      "source": [
        "# get me RAMMM!!!! \n",
        "import gc \n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR6oaYJ3gfaT"
      },
      "outputs": [],
      "source": [
        "# Create instances of AudioDataset for train-clean-360 and train-clean-100 partitions\n",
        "train_data = AudioDataset(root, phonemes=PHONEMES, partition=\"train-clean-360\")\n",
        "#train_data2 = AudioDataset(root, phonemes=PHONEMES, partition=\"train-clean-100\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhTSRxe6ESY5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a dataset object using the AudioDataset class for the validation data\n",
        "val_data = AudioDataset(root, phonemes=PHONEMES, partition=\"dev-clean\")\n",
        "\n",
        "# Create a dataset object using the AudioTestDataset class for the test data\n",
        "test_data = AudioDatasetTest(root, phonemes=PHONEMES, partition=\"test-clean\")\n",
        "\n",
        "# Create DataLoaders for train, validation, and test data\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn= train_data.collate_fn\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn= val_data.collate_fn\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn = test_data.collate_fn\n",
        ")\n",
        "\n",
        "# Print dataset and DataLoader information\n",
        "print(\"Batch size: \", BATCH_SIZE)\n",
        "print(f\"Train dataset samples = {len(train_data)}, batches = {len(train_loader)}\")\n",
        "print(f\"Val dataset samples = {len(val_data)}, batches = {len(val_loader)}\")\n",
        "print(f\"Test dataset samples = {len(test_data)}, batches = {len(test_loader)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG3_1UWaEbxp"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Tnj4ZXzEmHE"
      },
      "outputs": [],
      "source": [
        "# get me RAMMM!!!! \n",
        "import gc \n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K0Zvv6l3KHI"
      },
      "outputs": [],
      "source": [
        "# Utils for network\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class PermuteBlock(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        #print(x.shape)\n",
        "        \n",
        "        \n",
        "        return x.transpose(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QYKbVxDJGby"
      },
      "outputs": [],
      "source": [
        "class pBLSTM(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.blstm = torch.nn.LSTM(input_size=input_size*2, hidden_size=hidden_size, num_layers=2, bidirectional=True,batch_first =True)\n",
        "    \n",
        "    def forward(self, x_packed):\n",
        "        #print(\"In pblstm\")\n",
        "\n",
        "        x, x_lens = pad_packed_sequence(x_packed, batch_first = True) # Pad the packed sequence to get the padded tensor and the lengths tensor\n",
        "        #print(\"Shape of x after pad_packed_sequence:\", x.shape)\n",
        "        #print(\"Shape of x_lens after pad_packed_sequence:\", x_lens.shape)\n",
        "        \n",
        "        x, x_lens = self.trunc_reshape(x, x_lens) \n",
        "        #print(\"Shape of x after trunc_reshape:\", x.shape)\n",
        "        #print(\"Shape of x_lens after trunc_reshape:\", x_lens.shape)\n",
        "        \n",
        "        x_packed = pack_padded_sequence(x, x_lens, batch_first = True, enforce_sorted=False)\n",
        "        \n",
        "\n",
        "        x_packed, _ = self.blstm(x_packed) \n",
        "\n",
        "        return x_packed\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens):\n",
        "      batch_size, time_steps, features = x.size()\n",
        "\n",
        "      if time_steps % 2 != 0: \n",
        "        x = x[:, :-1, :] # Remove the last timestep\n",
        "        time_steps -= 1\n",
        "\n",
        "\n",
        "      x = x.reshape(batch_size, time_steps // 2, features*2) \n",
        "      x_lens = x_lens // 2 \n",
        "\n",
        "      return x, x_lens\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zdqy3IFhEwwa"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    '''\n",
        "    The Encoder takes utterances as inputs and returns latent feature representations\n",
        "    '''\n",
        "    def __init__(self, input_size, encoder_hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embedding = torch.nn.Sequential(\n",
        "            # Permute: (batch_size, sequence_length, features) -> (batch_size, features, sequence_length)\n",
        "            PermuteBlock(),\n",
        "            # 1D CNN layer\n",
        "            nn.Conv1d(input_size, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            \n",
        "            PermuteBlock()\n",
        "        ) \n",
        "  \n",
        "        self.pBLSTMs = torch.nn.Sequential(\n",
        "            pBLSTM(256, encoder_hidden_size),\n",
        "            #LockedDropout(),\n",
        "            pBLSTM(encoder_hidden_size * 2, encoder_hidden_size),\n",
        "            #LockedDropout(),\n",
        "            \n",
        "            \n",
        "        ) \n",
        "\n",
        "        #self.LSTMs = torch.nn.LSTM(256, encoder_hidden_size,num_layers =1, batch_first =True,bidirectional = True)\n",
        "            \n",
        "        \n",
        "         \n",
        "    def forward(self, x, x_lens):\n",
        "        #print(\"Shape of x in pblstm:Encoder:\",x.shape)\n",
        "        x = self.embedding(x)\n",
        "        #print(\"Shape of x after passing through embedding: Encoder:\",x.shape)\n",
        "        #print(\"Minimum of x_lens and max of x_lens:\", min(x_lens), max(x_lens))\n",
        "        #x = x.permute(0, 2, 1)\n",
        "        x_packed = pack_padded_sequence(x, x_lens, batch_first= True, enforce_sorted=False)\n",
        "        x_packed = self.pBLSTMs(x_packed)\n",
        "      \n",
        "        #print(\"Shape of x after passing through pblstm:x_packed:\", x_packed.shape)\n",
        "        encoder_outputs, encoder_lens = pad_packed_sequence(x_packed, batch_first = True)\n",
        "        #print(\"Checking for After pBLSTM\")\n",
        "        #print(torch.any(torch.isnan(encoder_outputs)))\n",
        "        #print(torch.any(torch.isnan(encoder_lens)))\n",
        "\n",
        "       \n",
        "        #print(torch.any(torch.isinf(encoder_outputs)))\n",
        "        #print(torch.any(torch.isinf(encoder_lens)))\n",
        "\n",
        "        #print(\"Shape of x after passing through pblstm and pad packed:\", encoder_outputs.shape)\n",
        "\n",
        "        return encoder_outputs, encoder_lens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC5i4p4x0r07"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9zksuCoE0lt"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, output_size=41):\n",
        "        super().__init__()\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "    torch.nn.Linear(embed_size * 2, 2048),\n",
        "    PermuteBlock(),\n",
        "    torch.nn.BatchNorm1d(2048),\n",
        "    PermuteBlock(),\n",
        "    torch.nn.Dropout(p=0.2),\n",
        "    torch.nn.GELU(),\n",
        "\n",
        "    torch.nn.Linear(2048, 1024),\n",
        "    PermuteBlock(),\n",
        "    torch.nn.BatchNorm1d(1024),\n",
        "    PermuteBlock(),\n",
        "    torch.nn.Dropout(p=0.2),\n",
        "    torch.nn.GELU(),\n",
        "\n",
        "    torch.nn.Linear(1024, 512),\n",
        "    PermuteBlock(),\n",
        "    torch.nn.BatchNorm1d(512),\n",
        "    PermuteBlock(),\n",
        "    torch.nn.Dropout(p=0.2),\n",
        "    torch.nn.GELU(),\n",
        "\n",
        "    torch.nn.Linear(512, output_size),\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, encoder_out):\n",
        "        # Call your MLP\n",
        "        #print(\"Shape of x before MLP:\", x.shape)\n",
        "        mlp_out = self.mlp(encoder_out)\n",
        "        \n",
        "        \n",
        "        #print(\"Shape of x after MLP:\", x.shape)\n",
        "\n",
        "        # Apply LogSoftmax to the final output of the decoder for the classification\n",
        "        out = self.softmax(mlp_out)\n",
        "        \n",
        "        #print(\"Shape of x after softmax:\",x.shape)\n",
        "\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BchGbMXmeGvP"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "\n",
        "class ASRModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, embed_size=192, output_size=len(PHONEMES)):\n",
        "        super().__init__()\n",
        "\n",
        "        self.augmentations = torch.nn.Sequential(\n",
        "            PermuteBlock(),\n",
        "        \n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=200, iid_masks=True),\n",
        "\n",
        "            #PermuteBlock(),\n",
        "         \n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param= 5 , iid_masks=True),\n",
        "\n",
        "            PermuteBlock()\n",
        ")\n",
        "        \n",
        "        self.encoder = Encoder(input_size, embed_size*2)\n",
        "        self.decoder = Decoder(embed_size*2, output_size)\n",
        "\n",
        "    def forward(self, x, lengths_x):\n",
        "        if self.training:\n",
        "            x = self.augmentations(x)\n",
        "\n",
        "        encoder_out, encoder_lens = self.encoder(x, lengths_x)\n",
        "        #print(\"Checking for Encoder\")\n",
        "        #print(torch.any(torch.isnan(encoder_out)))\n",
        "        #print(torch.any(torch.isnan(encoder_lens)))\n",
        "        #print(torch.any(torch.isinf(encoder_out)))\n",
        "        #print(torch.any(torch.isinf(encoder_lens)))\n",
        "\n",
        "        decoder_out = self.decoder(encoder_out)\n",
        "        #print(\"Checking for Decoder\")\n",
        "        #print(torch.any(torch.isnan(decoder_out)))\n",
        "        #print(torch.any(torch.isnan(encoder_lens)))\n",
        "        \n",
        "        #print(torch.any(torch.isinf(decoder_out)))\n",
        "        #print(torch.any(torch.isinf(encoder_lens)))\n",
        "\n",
        "\n",
        "\n",
        "        return decoder_out, encoder_lens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrTwbw4W0uuK"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyAmiiu_hRt8"
      },
      "outputs": [],
      "source": [
        "from torchsummaryX import summary\n",
        "\n",
        "model = ASRModel(\n",
        "    input_size  = 27,\n",
        "    embed_size  = 128,\n",
        "    output_size = len(PHONEMES)\n",
        ").to(device)\n",
        "\n",
        "print(model)\n",
        "summary(model, x.to(device), lx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhwoboztE7nq"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"beam_width\" : 3,\n",
        "    \"lr\" : 1e-4,\n",
        "    \"epochs\" : 70\n",
        "    } # Feel free to add more items here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gUBf4ZQFA73"
      },
      "outputs": [],
      "source": [
        "# Define CTC loss as the criterion. How would the losses be reduced?\n",
        "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "# Refer to the handout for hints\n",
        "criterion = nn.CTCLoss(blank=0, reduction='mean')\n",
        "\n",
        "# What goes in here?\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-5)\n",
        "\n",
        "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
        "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
        "# Here is a possible way to create the decoder, but please look up documentation to see what options are available.\n",
        "\n",
        "decoder = ctcdecode.CTCBeamDecoder(LABELS,beam_width=config['beam_width'], blank_id = 0,log_probs_input = True)\n",
        "\n",
        "# Learning rate scheduler, if you need it\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MkiROwGUPht"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def decode_prediction(output, output_lens, decoder, PHONEME_MAP=LABELS):\n",
        "    # output = torch.permute(output,(1,0,2))\n",
        "    \n",
        "    #print(\"Shape of output:\", output.shape)\n",
        "    \n",
        "    beam_results,beam_scores, timesteps, out_seq_len = decoder.decode(output, seq_lens=output_lens)\n",
        "    #print(\"Beam results:\", beam_results[0][0].shape)\n",
        "\n",
        "    pred_strings = []\n",
        "    \n",
        "    for i in range(output_lens.shape[0]):\n",
        "        prediction = beam_results[i][0][:out_seq_len[i][0]]\n",
        "        \n",
        "        pred_string = \"\".join(PHONEME_MAP[prediction[k]] for k in range(len(prediction)))\n",
        "       \n",
        "        pred_strings.append(pred_string)\n",
        "    \n",
        "    return pred_strings\n",
        "\n",
        "\n",
        "def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP=LABELS):\n",
        "    \n",
        "    dist = 0\n",
        "    batch_size = label.shape[0]\n",
        "\n",
        "    pred_strings = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        pred_string = pred_strings[i]\n",
        "        label_string = \"\".join(PHONEME_MAP[label[i][k]] for k in range(label_lens[i]))\n",
        "        dist += Levenshtein.distance(pred_string, label_string)\n",
        "\n",
        "    dist /= batch_size\n",
        "    return dist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOk4pYf02N9B"
      },
      "outputs": [],
      "source": [
        "# test code to check shapes\n",
        "\n",
        "model.eval()\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    x, y, lx, ly = data\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    \n",
        "  \n",
        "    #print(type(lx))\n",
        "    #with torch.no_grad():\n",
        "    h,lh= model(x,lx)\n",
        "    #h, lh = model(x, lx)\n",
        "    #print(h.type)\n",
        "    #lh = torch.full(size=(h.shape[0],), fill_value= 1, dtype=torch.long)\n",
        "    #lh = lh.to(torch.int64)  # Convert lh to an integral dtype\n",
        "    #print(\"Shape of input lenght:\",lh.shape)\n",
        "   \n",
        "\n",
        "    loss = criterion(torch.permute(h, (1, 0, 2)), y, lh, ly)\n",
        "    #h = torch.permute(h, (1, 0, 2))\n",
        "    #print(loss)\n",
        "    #lh = torch.full(size=(h.shape[0],), fill_value=h.shape[0], dtype=torch.long)\n",
        "\n",
        "    #print(lh.shape)\n",
        "    \n",
        "\n",
        "    #loss = criterion(h, ly, lh, ly)\n",
        "    #print(loss.item())\n",
        "\n",
        "           \n",
        "    #lh = torch.full(size=(h.shape[1],), fill_value= 1, dtype=torch.long)\n",
        "           \n",
        "    #h = h.unsqueeze(0) # add an extra dimension to h\n",
        "    \n",
        "    #print(\"Before conversion:\")\n",
        "    #print(\"input_lengths shape:\", lh.shape, \"dtype:\", lh.dtype)\n",
        "    #print(\"target_lengths shape:\", ly.shape, \"dtype:\", ly.dtype)\n",
        "\n",
        "    # Checking the input_lengths and target_lengths are of shape [batch_size]\n",
        "    lh = lh.squeeze()\n",
        "    ly = ly.squeeze()\n",
        "\n",
        "   # Checking the input_lengths and target_lengths are of type torch.long\n",
        "    lh = lh.long()\n",
        "    ly = ly.long()\n",
        "\n",
        "    #print(\"After conversion:\")\n",
        "    #print(\"input_lengths shape:\", lh.shape, \"dtype:\", lh.dtype)\n",
        "    #print(\"target_lengths shape:\", ly.shape, \"dtype:\", ly.dtype)\n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "    #print(decode_prediction(h,lx , decoder, LABELS))\n",
        "    print(calculate_levenshtein(h, y, lh, ly, decoder, LABELS))\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF7YqX-pFK2Z"
      },
      "outputs": [],
      "source": [
        "wandb.login(key=\"a0b5e594abd47863b3c9a38a1319b7dbecb14246\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEx5mZCEFPHO"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    name = \"early-submission1\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw3p2-ablations\", ### Project should be created in your wandb account \n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oFIYpbeFTmP"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "    \n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast(): \n",
        "           h,lh = model(x,lx)\n",
        "           #print(\"In main train loop\")\n",
        "           #print(torch.any(torch.isnan(h)))\n",
        "           #print(torch.any(torch.isnan(lh)))\n",
        "           #print(torch.any(torch.isinf(h)))\n",
        "           #print(torch.any(torch.isinf(lh)))\n",
        "           #h, lh = model(x, lx)\n",
        "           #print(h.shape)\n",
        "           #lh = torch.full(size=(h.shape[0],), fill_value= 1, dtype=torch.long)\n",
        "           #lh = lh.to(torch.int64)  # Convert lh to an integral dtype\n",
        "           #print(\"Shape of input lenght:\",lh.shape)\n",
        "   \n",
        "\n",
        "           loss = criterion(torch.permute(h, (1, 0, 2)), y, lh, ly)\n",
        "           #print(loss)\n",
        "\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        # Another couple things you need for FP16. \n",
        "        #scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        #scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        #scaler.update() # This is something added just for FP16\n",
        "        \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update()\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss \n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    vdist = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        \n",
        "        \n",
        "        with torch.inference_mode():\n",
        "           h, lh = model(x,lx)\n",
        "           #h, lh = model(x, lx)\n",
        "           #print(h.shape)\n",
        "           #lh = torch.full(size=(h.shape[0],), fill_value= 1, dtype=torch.long)\n",
        "           \n",
        "           #lh = lh.to(torch.int64)  # Convert lh to an integral dtype\n",
        "           #print(\"Shape of input lenght:\",lh.shape)\n",
        "   \n",
        "\n",
        "           loss = criterion(torch.permute(h, (1, 0, 2)), y, lh, ly)\n",
        "\n",
        "      \n",
        "\n",
        "        total_loss += float(loss)\n",
        "        vdist += calculate_levenshtein(h, y, lh, ly, decoder, phoneme_map)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "    \n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    val_dist = vdist/len(val_loader)\n",
        "    return total_loss, val_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4W6XSpyvB99"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1], \n",
        "         'epoch'                    : epoch}, \n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        \n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuTrJp41vLQV"
      },
      "outputs": [],
      "source": [
        "\n",
        "last_epoch_completed = 45\n",
        "start = last_epoch_completed\n",
        "end = config[\"epochs\"]\n",
        "best_lev_dist = 4.1598\n",
        "epoch_model_path = '/content/gdrive/MyDrive/checkpoint'\n",
        "best_model_path = '/content/gdrive/MyDrive/checkpoint'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOMY2Rm5Fn_d"
      },
      "outputs": [],
      "source": [
        "for epoch in range(config['epochs']):\n",
        "  print(\"\\nEpoch {}/{}\".format(epoch+1,config['epochs']))\n",
        "    \n",
        "  curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "  train_loss = train_model(model, train_loader, criterion, optimizer)\n",
        "  valid_loss, valid_dist = validate_model(model, val_loader, decoder, LABELS)\n",
        "  scheduler.step(valid_dist)\n",
        "  print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "  print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
        "    \n",
        "\n",
        "  \n",
        "\n",
        "  wandb.log({\n",
        "        'train_loss': train_loss,  \n",
        "        'valid_dist': valid_dist, \n",
        "        'valid_loss': valid_loss, \n",
        "        'lr': curr_lr,\n",
        "        \n",
        "    })\n",
        "    \n",
        "  #save_model(model, optimizer, scheduler, ['valid_dist', val_dist], epoch, epoch_model_path)\n",
        "  #wandb.save(epoch_model_path)\n",
        "  #print(\"Saved epoch model\")\n",
        "\n",
        "  #if val_dist <= best_lev_dist:\n",
        "    #best_lev_dist = val_dist\n",
        "    #save_model(model, optimizer, scheduler, ['valid_dist', val_dist], epoch, best_model_path)\n",
        "    #wandb.save(best_model_path)\n",
        "    #print(\"Saved best model\")\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcaDeiwOOODZ"
      },
      "outputs": [],
      "source": [
        "valid_loss, valid_dist = validate_model(model, val_loader, decoder, LABELS)\n",
        "print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Epj5cCeVIUC4"
      },
      "outputs": [],
      "source": [
        "#TODO: Make predictions\n",
        "\n",
        "# Follow the steps below:\n",
        "# 1. Create a new object for CTCBeamDecoder with a larger number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "\n",
        "TEST_BEAM_WIDTH = 10 # Use a larger beam width to improve the search for the best path\n",
        "\n",
        "test_decoder = CTCBeamDecoder(\n",
        "    labels=LABELS,\n",
        "    beam_width=TEST_BEAM_WIDTH,\n",
        "    log_probs_input=True,\n",
        ")\n",
        "results = []\n",
        "\n",
        "model.eval()\n",
        "print(\"Testing\")\n",
        "for data in tqdm(test_loader):\n",
        "\n",
        "    x , lx= data\n",
        "    \n",
        "    x = x.to(device)\n",
        "    \n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "     \n",
        "      h, lh = model(x,lx)\n",
        "      #h = torch.permute(h, (1, 0, 2))\n",
        "      prediction_string = decode_prediction(h, lh, test_decoder, LABELS)\n",
        "      results.extend(prediction_string)  \n",
        "    \n",
        "      del x, lx, h, lh\n",
        "      torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zxTC0PblVqP"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/11-785-s23-hw3p2/test-clean/random_submission.csv\"\n",
        "df = pd.read_csv(data_dir)\n",
        "df.label = results\n",
        "df.to_csv('submission.csv', index = False)\n",
        "! kaggle competitions submit -c automatic-speech-recognition-asr-slack-kaggle -f submission.csv -m \"Message\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}